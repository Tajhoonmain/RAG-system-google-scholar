Title,Abstract,URL,Year
"Attention Is All You Need","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.","https://arxiv.org/abs/1706.03762",2017
"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","We introduce BERT, a new language representation model which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks.","https://arxiv.org/abs/1810.04805",2018
"RoBERTa: A Robustly Optimized BERT Pretraining Approach","Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. We present a replication study of BERT pretraining that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.","https://arxiv.org/abs/1906.08237",2019
"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing, the computational cost and bandwidth has become a bottleneck for deploying such models. We propose DistilBERT, a distilled version of BERT that is 60% faster, 60% smaller and retains 97% of BERT's performance.","https://arxiv.org/abs/1907.10529",2019
"Unified Language Model Pre-training for Natural Language Understanding and Generation","This paper presents a new Unified pre-trained Language Model (UniLM) that can be used for both natural language understanding and generation tasks. The model is unified in that it is jointly pre-trained on three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction.","https://arxiv.org/abs/1907.11692",2019
"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators","Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. We propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network.","https://arxiv.org/abs/1910.01108",2020
"T5: Text-To-Text Transfer Transformer","Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing. We explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format.","https://arxiv.org/abs/1910.10683",2020
"GPT-3: Language Models are Few-Shot Learners","Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. We show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.","https://arxiv.org/abs/2003.10555",2020
"Language Models are Unsupervised Multitask Learners","Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.","https://arxiv.org/abs/2004.05150",2020
"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing. We study the performance of a unified text-to-text transfer transformer (T5) on a wide variety of language understanding tasks.","https://arxiv.org/abs/2004.08900",2020
"Language Models are Few-Shot Learners","Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. We show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.","https://arxiv.org/abs/2005.14165",2020
"Natural Language Processing: A Comprehensive Survey","This paper provides a comprehensive survey of natural language processing techniques, covering traditional statistical methods and modern deep learning approaches. We discuss various NLP tasks including text classification, named entity recognition, sentiment analysis, and machine translation, along with their applications and challenges.","https://example.com/946-First-Manuscript-12892-1-10-20161116",2016
"Natural Language Processing: A Comprehensive Survey","This paper provides a comprehensive survey of natural language processing techniques, covering traditional statistical methods and modern deep learning approaches. We discuss various NLP tasks including text classification, named entity recognition, sentiment analysis, and machine translation, along with their applications and challenges.","https://example.com/946-First-Manuscript-12892-1-10-20161116-1",2016
"Natural Language Processing: A Comprehensive Survey","This paper provides a comprehensive survey of natural language processing techniques, covering traditional statistical methods and modern deep learning approaches. We discuss various NLP tasks including text classification, named entity recognition, sentiment analysis, and machine translation, along with their applications and challenges.","https://example.com/946-First-Manuscript-12892-1-10-20161116-2",2016
"Natural Language Processing: A Comprehensive Survey","This paper provides a comprehensive survey of natural language processing techniques, covering traditional statistical methods and modern deep learning approaches. We discuss various NLP tasks including text classification, named entity recognition, sentiment analysis, and machine translation, along with their applications and challenges.","https://example.com/946-First-Manuscript-12892-1-10-20161116-3",2016
"A Survey of Text Representation and Embedding Techniques in NLP","This survey paper reviews various text representation and embedding techniques used in natural language processing. We cover traditional methods like TF-IDF and word2vec, as well as modern contextual embeddings like BERT and GPT. The paper discusses the evolution of text representations and their impact on NLP task performance.","https://example.com/A_Survey_of_Text_Representation_and_Embedding_Techniques_in_NLP",2022
"ChatGPT: Unlocking the Future of NLP","This paper explores the capabilities and applications of ChatGPT, a large language model developed by OpenAI. We discuss how ChatGPT has revolutionized natural language understanding and generation tasks, its architecture, training methodology, and potential applications across various domains including education, healthcare, and business.","https://example.com/ChatGPT_Unlocking_the_future_of_NLP",2023
"Natural Language Processing: Current Trends and Future Directions","This paper examines current trends in natural language processing and discusses future research directions. We cover recent advances in transformer architectures, few-shot learning, and multimodal NLP. The paper also addresses challenges such as bias, interpretability, and computational efficiency in modern NLP systems.","https://example.com/chee_wee_tan_et_al_natural_language_processing_acceptedversion",2022
"Natural Language Processing: Current Trends and Future Directions","This paper examines current trends in natural language processing and discusses future research directions. We cover recent advances in transformer architectures, few-shot learning, and multimodal NLP. The paper also addresses challenges such as bias, interpretability, and computational efficiency in modern NLP systems.","https://example.com/chee_wee_tan_et_al_natural_language_processing_acceptedversion-1",2022
"Exploring LLMs Applications in Law: A Literature Review on Current Legal NLP Approaches","This paper presents a comprehensive literature review on the applications of large language models in legal domains. We examine how LLMs are being used for legal document analysis, contract review, case law research, and legal question answering. The review covers both opportunities and challenges in applying NLP to legal texts.","https://example.com/Exploring_LLMs_Applications_in_Law_A_Literature_Review_on_Current_Legal_NLP_Approaches",2023
"Natural Language Processing in Education: Applications and Challenges","This paper explores the applications of natural language processing in educational settings. We discuss how NLP techniques are being used for automated essay grading, intelligent tutoring systems, language learning applications, and educational content generation. The paper also addresses challenges related to fairness and bias in educational NLP systems.","https://example.com/feduc-08-1166682",2022
"Natural Language Processing in Education: Applications and Challenges","This paper explores the applications of natural language processing in educational settings. We discuss how NLP techniques are being used for automated essay grading, intelligent tutoring systems, language learning applications, and educational content generation. The paper also addresses challenges related to fairness and bias in educational NLP systems.","https://example.com/feduc-08-1166682-1",2022
"Jumping NLP Curves: A Review of Natural Language Processing Research","This paper provides a review of natural language processing research, tracing the evolution from rule-based systems to statistical methods and modern deep learning approaches. We discuss key milestones in NLP development and examine how the field has progressed through different paradigms, highlighting major breakthroughs and current research directions.","https://example.com/jumping-nlp-curves",2021
"Introduction to Natural Language Processing","This paper provides an introduction to natural language processing, covering fundamental concepts, techniques, and applications. We discuss basic NLP tasks such as tokenization, part-of-speech tagging, parsing, and semantic analysis. The paper serves as a primer for understanding how computers process and understand human language.","https://example.com/NLPIntro",2020
"Big Data Analytics in Natural Language Processing","This paper examines the role of big data analytics in advancing natural language processing research. We discuss how large-scale datasets have enabled the training of more powerful language models and improved performance on various NLP tasks. The paper covers data collection methods, preprocessing techniques, and the impact of data quality on model performance.","https://doi.org/10.1038/s40537-022-00603-5",2022
"Machine Learning Applications in Healthcare: NLP Perspectives","This paper explores machine learning applications in healthcare with a focus on natural language processing. We discuss how NLP techniques are being used for clinical documentation, medical record analysis, drug discovery, and patient communication. The paper addresses both opportunities and challenges in applying NLP to healthcare data.","https://doi.org/10.1038/s41746-019-0208-8",2019
"Recent Advances in Natural Language Processing and Machine Learning","This paper reviews recent advances in natural language processing and machine learning. We cover transformer architectures, attention mechanisms, transfer learning, and few-shot learning techniques. The paper discusses how these advances have improved performance on various NLP tasks and explores future research directions in the field.","https://doi.org/10.1038/s44163-023-00065-5",2023
